---
blueprint_name: smd-s-vm-1a

vars:
  project_id: hpc-in-gcp
  host_project_id: ris-network-host-dev
  deployment_name: smd-s-vm-1a
  region: us-central1
  zone: us-central1-c
  custom_tags: ["icmp", "http-https", "ssh", "hpc-in-gcp"]
  instance_image:
    # Please refer to the following link for the latest images:
    # https://github.com/SchedMD/slurm-gcp/blob/master/docs/images.md#supported-operating-systems
    # In case need to change the images
    family: slurm-gcp-5-9-ubuntu-2004-lts
    project: schedmd-slurm-public

deployment_groups:
- group: primary
  modules:
  - id: network1
    source: modules/network/pre-existing-vpc
    settings:
      project_id: $(vars.host_project_id)
      network_name: ris-network-host-dev-network
      subnetwork_name: hpc-in-gcp-subnetwork

  #Scratch storage space  (Temp scratch space)
  - id: gcp_scratch
    source: modules/file-system/filestore
    use: [network1]
    settings:
      project_id: $(vars.project_id)
      filestore_tier: BASIC_SSD
      size_gb: 4096
      local_mount: /storage1/fs1/hpc-cloud-poc/Active
      connect_mode: PRIVATE_SERVICE_ACCESS

  #Pre staged object storage (home storage)
  #Ensure bucket is created before "gcp_object_storage"
  - id: gcp_object_storage
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: gcp_object_storage
      local_mount: /gcp_object_storage
      fs_type: gcsfuse
      mount_options: defaults,_netdev,implicit_dirs

  - id: gcp_hpc_cloup_poc_backup
    source: modules/file-system/pre-existing-network-storage
    settings:
      remote_mount: hpc-cloud-poc-backup
      fs_type: gcsfuse
      local_mount: /data
      mount_options: defaults,_netdev,implicit_dirs,allow_other

  #CPU preemtable nodegroup and queue
  - id: general
    source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
    settings:
      node_count_dynamic_max: 100
      machine_type: custom-20-131072
      preemptible: true
      tags: $(vars.custom_tags)
      guest_accelerator:
      - type: nvidia-tesla-v100
        count: 2


  - id: general_partition
    source: community/modules/compute/schedmd-slurm-gcp-v5-partition
    use:
    - network1
    - gcp_scratch
    - gcp_hpc_cloup_poc_backup
    - general
    - startup-script
    settings:
      partition_name: general
      enable_placement: false
      exclusive: false
      is_default: true
      partition_conf:
        Oversubscribe: NO


  #SLURM controller and client
  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v5-controller
    use:
    - network1
    - general_partition
    - gcp_scratch
    - gcp_hpc_cloup_poc_backup
    - startup-script
    settings:
      disable_controller_public_ips: true
      tags: $(vars.custom_tags)

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v5-login
    use:
    - network1
    - slurm_controller
    settings:
      machine_type: n2-standard-4
      disable_login_public_ips: true
      tags: $(vars.custom_tags)


  - id: startup-script
    source: modules/scripts/startup-script
    settings:
      runners:
      - type: shell
        destination: "/tmp/mysetup.sh"
        content: |
          #!/bin/bash
          rm -f /etc/apt/sources.list.d/gcsfuse.list
          perl -i -p -ne 's/"1"/"0"/g' /etc/apt/apt.conf.d/20auto-upgrades
          curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
          && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
          sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
          tee /etc/apt/sources.list.d/nvidia-container-toolkit.list && \
          apt-get update
          apt-get install -y nvidia-container-toolkit primesieve docker.io ubuntu-drivers-common
          chmod a+s /usr/bin/docker

